{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JapiKredi/Faiss_HuggingFace_PDF/blob/main/updating_index_in_faiss_db.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence-transformers\n",
        "!pip install pypdf\n",
        "!pip install pandas\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7VtTKjwWK0h",
        "outputId": "c058a951-4ba5-4ae8-d29d-1f26db2b160f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.2)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.32 (from langchain)\n",
            "  Downloading langchain_core-0.2.33-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.99-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.32->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (4.12.2)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading langchain-0.2.14-py3-none-any.whl (997 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.33-py3-none-any.whl (391 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m391.5/391.5 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.99-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: tenacity, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.14 langchain-core-0.2.33 langchain-text-splitters-0.2.2 langsmith-0.1.99 orjson-3.10.7 tenacity-8.5.0\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Downloading faiss_cpu-1.8.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0.post1\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 sentence-transformers-3.0.1\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.3.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "2Acs87RIVqwo",
        "outputId": "8d79a900-c007-43b7-e1a7-f6aac2e7a3cd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3c2b84ab6708>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRetrievalQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_splitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnxKpdAcVqwp"
      },
      "source": [
        "### 1. Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enI1qwtoVqwp"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(\"yolo.pdf\")\n",
        "documents = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9725pu75Vqwq",
        "outputId": "b8fa5891-822e-4fae-8a01-1e1805938124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents length -- 16\n",
            "You Only Look Once:\n",
            "Uniﬁed, Real-Time Object Detection\n",
            "Joseph Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗†\n",
            "University of Washington∗, Allen Institute for AI†, Facebook AI Research¶\n",
            "http://pjreddie.com/yolo/\n",
            "Abstract\n",
            "We present YOLO, a new approach to object detection.\n",
            "Prior work on object detection repurposes classiﬁers to per-\n",
            "form detection. Instead, we frame object detection as a re-\n",
            "gression problem to spatially separated bounding boxes and\n",
            "associated class probabilities. A single neural network pre-\n",
            "dicts bounding boxes and class probabilities directly from\n",
            "full images in one evaluation. Since the whole detection\n",
            "pipeline is a single network, it can be optimized end-to-end\n",
            "directly on detection performance.\n",
            "Our uniﬁed architecture is extremely fast. Our base\n",
            "YOLO model processes images in real-time at 45 frames\n",
            "per second. A smaller version of the network, Fast YOLO,\n",
            "processes an astounding 155 frames per second while\n",
            "still achieving double the mAP of other real-time detec-\n",
            "tors. Compared to state-of-the-art detection systems, YOLO\n",
            "makes more localization errors but is less likely to predict\n",
            "false positives on background. Finally, YOLO learns very\n",
            "general representations of objects. It outperforms other de-\n",
            "tection methods, including DPM and R-CNN, when gener-\n",
            "alizing from natural images to other domains like artwork.\n",
            "1. Introduction\n",
            "Humans glance at an image and instantly know what ob-\n",
            "jects are in the image, where they are, and how they inter-\n",
            "act. The human visual system is fast and accurate, allow-\n",
            "ing us to perform complex tasks like driving with little con-\n",
            "scious thought. Fast, accurate algorithms for object detec-\n",
            "tion would allow computers to drive cars without special-\n",
            "ized sensors, enable assistive devices to convey real-time\n",
            "scene information to human users, and unlock the potential\n",
            "for general purpose, responsive robotic systems.\n",
            "Current detection systems repurpose classiﬁers to per-\n",
            "form detection. To detect an object, these systems take a\n",
            "classiﬁer for that object and evaluate it at various locations\n",
            "and scales in a test image. Systems like deformable parts\n",
            "models (DPM) use a sliding window approach where the\n",
            "classiﬁer is run at evenly spaced locations over the entire\n",
            "image [10].\n",
            "More recent approaches like R-CNN use region proposal\n",
            "1. Resize image.\n",
            "2. Run convolutional network.3. Non-max suppression.\n",
            "Dog: 0.30Person: 0.64Horse: 0.28Figure 1: The YOLO Detection System. Processing images\n",
            "with YOLO is simple and straightforward. Our system (1) resizes\n",
            "the input image to 448×448, (2) runs a single convolutional net-\n",
            "work on the image, and (3) thresholds the resulting detections by\n",
            "the model’s conﬁdence.\n",
            "methods to ﬁrst generate potential bounding boxes in an im-\n",
            "age and then run a classiﬁer on these proposed boxes. After\n",
            "classiﬁcation, post-processing is used to reﬁne the bound-\n",
            "ing boxes, eliminate duplicate detections, and rescore the\n",
            "boxes based on other objects in the scene [13]. These com-\n",
            "plex pipelines are slow and hard to optimize because each\n",
            "individual component must be trained separately.\n",
            "We reframe object detection as a single regression prob-\n",
            "lem, straight from image pixels to bounding box coordi-\n",
            "nates and class probabilities. Using our system, you only\n",
            "look once (YOLO) at an image to predict what objects are\n",
            "present and where they are.\n",
            "YOLO is refreshingly simple: see Figure 1. A sin-\n",
            "gle convolutional network simultaneously predicts multi-\n",
            "ple bounding boxes and class probabilities for those boxes.\n",
            "YOLO trains on full images and directly optimizes detec-\n",
            "tion performance. This uniﬁed model has several beneﬁts\n",
            "over traditional methods of object detection.\n",
            "First, YOLO is extremely fast. Since we frame detection\n",
            "as a regression problem we don’t need a complex pipeline.\n",
            "We simply run our neural network on a new image at test\n",
            "time to predict detections. Our base network runs at 45\n",
            "frames per second with no batch processing on a Titan X\n"
          ]
        }
      ],
      "source": [
        "print(f'Documents length -- {len(documents)}')\n",
        "print(documents[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvxYy0ioVqwq"
      },
      "source": [
        "### 2. Splitting the documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8EaDQ7AVqwq"
      },
      "outputs": [],
      "source": [
        "split_text = RecursiveCharacterTextSplitter(chunk_size =512, chunk_overlap = 50)\n",
        "texts = split_text.split_documents(documents=documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsVsjWPrVqwr",
        "outputId": "64c91dcd-a287-4f76-a383-64bae7ef4148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You Only Look Once:\n",
            "Uniﬁed, Real-Time Object Detection\n",
            "Joseph Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗†\n",
            "University of Washington∗, Allen Institute for AI†, Facebook AI Research¶\n",
            "http://pjreddie.com/yolo/\n",
            "Abstract\n",
            "We present YOLO, a new approach to object detection.\n",
            "Prior work on object detection repurposes classiﬁers to per-\n",
            "form detection. Instead, we frame object detection as a re-\n",
            "gression problem to spatially separated bounding boxes and\n"
          ]
        }
      ],
      "source": [
        "print(texts[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNM27AneVqwr"
      },
      "source": [
        "### 3. Embedding the documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WqU3OfpVqwr",
        "outputId": "0843a759-8bf3-4f1c-b6ea-16b9093fd9e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\LENOVO\\sagar\\work\\AI_ML\\gitrepo\\GENAI\\YOUTUBES\\Talk_with_your_data\\local_GPT_chatbot\\localgpt_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "    model_kwargs={'device':'cpu'}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wvp1G4CVqwr"
      },
      "source": [
        "### 4. Creating vector db index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N63fISqFVqwr"
      },
      "outputs": [],
      "source": [
        "faiss_index = FAISS.from_documents(texts, embeddings)\n",
        "faiss_index_name = 'faiss_index_updated'\n",
        "\n",
        "faiss_index.save_local(faiss_index_name) # a directory shall be created in the name of <faiss_index_name> and couple of files are created"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrUk2QYdVqwr"
      },
      "source": [
        "### 5. Loading the vector db from local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKx10FwNVqwr"
      },
      "outputs": [],
      "source": [
        "loaded_vector_db_from_local = FAISS.load_local('./faiss_index_updated', embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXDTwlrcVqws"
      },
      "source": [
        "### 6. Viewing the vector db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHLak0_FVqws"
      },
      "outputs": [],
      "source": [
        "def convert_vectordb_to_df(vectorDb):\n",
        "    vector_dict = vectorDb.docstore._dict\n",
        "    data_rows = []\n",
        "\n",
        "    for k in vector_dict.keys():\n",
        "        doc_name = vector_dict[k].metadata['source'].split('/')[-1]\n",
        "        page_number = vector_dict[k].metadata['page'] + 1\n",
        "        content =  vector_dict[k].page_content\n",
        "        data_rows.append({\"chunk_id\": k, \"document\": doc_name, \"page\": page_number, \"content\":content})\n",
        "\n",
        "    vector_df = pd.DataFrame(data_rows)\n",
        "    print(vector_df)\n",
        "    return vector_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3k6qS2kVqws",
        "outputId": "95d68a28-68ba-4d9f-c13e-bc95f5c6ad66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                chunk_id  document  page  \\\n",
            "0   75337173-28cd-44ca-b57f-6b38c307a6e1  yolo.pdf     1   \n",
            "1   ec546c79-8340-445f-ad01-b42d1db9e5b3  yolo.pdf     1   \n",
            "2   2ce49afc-d253-43fc-95e0-f80e6ba3d2a1  yolo.pdf     1   \n",
            "3   71bccb92-f842-44f7-b49c-efa8dc0ace50  yolo.pdf     1   \n",
            "4   d0d2901e-8048-46c3-be4a-84ffe589717b  yolo.pdf     1   \n",
            "..                                   ...       ...   ...   \n",
            "94  b3b09bf4-aba4-4fef-bbbb-d42389b344ce  yolo.pdf     9   \n",
            "95  5d01ef25-bbad-46d9-b0e2-917796c7d3d4  yolo.pdf     9   \n",
            "96  c0fe7989-80c7-4519-99f9-7f45325d1a7f  yolo.pdf    10   \n",
            "97  51341021-c471-48ae-b2b8-29dcb72486d9  yolo.pdf    10   \n",
            "98  cd987d5a-e5c6-4708-a1b4-9b9202156266  yolo.pdf    10   \n",
            "\n",
            "                                              content  \n",
            "0   You Only Look Once:\\nUniﬁed, Real-Time Object ...  \n",
            "1   associated class probabilities. A single neura...  \n",
            "2   still achieving double the mAP of other real-t...  \n",
            "3   jects are in the image, where they are, and ho...  \n",
            "4   for general purpose, responsive robotic system...  \n",
            "..                                                ...  \n",
            "94  5\\n[28] S. Ren, K. He, R. Girshick, and J. Sun...  \n",
            "95  Recognition Challenge. International Journal o...  \n",
            "96  [33] Z. Shen and X. Xue. Do more dropouts in p...  \n",
            "97  4\\n[36] P. Viola and M. Jones. Robust real-tim...  \n",
            "98  2497–2504. IEEE, 2014. 5, 6\\n[39] C. L. Zitnic...  \n",
            "\n",
            "[99 rows x 4 columns]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunk_id</th>\n",
              "      <th>document</th>\n",
              "      <th>page</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>75337173-28cd-44ca-b57f-6b38c307a6e1</td>\n",
              "      <td>yolo.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>You Only Look Once:\\nUniﬁed, Real-Time Object ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ec546c79-8340-445f-ad01-b42d1db9e5b3</td>\n",
              "      <td>yolo.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>associated class probabilities. A single neura...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2ce49afc-d253-43fc-95e0-f80e6ba3d2a1</td>\n",
              "      <td>yolo.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>still achieving double the mAP of other real-t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>71bccb92-f842-44f7-b49c-efa8dc0ace50</td>\n",
              "      <td>yolo.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>jects are in the image, where they are, and ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>d0d2901e-8048-46c3-be4a-84ffe589717b</td>\n",
              "      <td>yolo.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>for general purpose, responsive robotic system...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>b3b09bf4-aba4-4fef-bbbb-d42389b344ce</td>\n",
              "      <td>yolo.pdf</td>\n",
              "      <td>9</td>\n",
              "      <td>5\\n[28] S. Ren, K. He, R. Girshick, and J. Sun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>5d01ef25-bbad-46d9-b0e2-917796c7d3d4</td>\n",
              "      <td>yolo.pdf</td>\n",
              "      <td>9</td>\n",
              "      <td>Recognition Challenge. International Journal o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>c0fe7989-80c7-4519-99f9-7f45325d1a7f</td>\n",
              "      <td>yolo.pdf</td>\n",
              "      <td>10</td>\n",
              "      <td>[33] Z. Shen and X. Xue. Do more dropouts in p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>51341021-c471-48ae-b2b8-29dcb72486d9</td>\n",
              "      <td>yolo.pdf</td>\n",
              "      <td>10</td>\n",
              "      <td>4\\n[36] P. Viola and M. Jones. Robust real-tim...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>cd987d5a-e5c6-4708-a1b4-9b9202156266</td>\n",
              "      <td>yolo.pdf</td>\n",
              "      <td>10</td>\n",
              "      <td>2497–2504. IEEE, 2014. 5, 6\\n[39] C. L. Zitnic...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                chunk_id  document  page  \\\n",
              "0   75337173-28cd-44ca-b57f-6b38c307a6e1  yolo.pdf     1   \n",
              "1   ec546c79-8340-445f-ad01-b42d1db9e5b3  yolo.pdf     1   \n",
              "2   2ce49afc-d253-43fc-95e0-f80e6ba3d2a1  yolo.pdf     1   \n",
              "3   71bccb92-f842-44f7-b49c-efa8dc0ace50  yolo.pdf     1   \n",
              "4   d0d2901e-8048-46c3-be4a-84ffe589717b  yolo.pdf     1   \n",
              "..                                   ...       ...   ...   \n",
              "94  b3b09bf4-aba4-4fef-bbbb-d42389b344ce  yolo.pdf     9   \n",
              "95  5d01ef25-bbad-46d9-b0e2-917796c7d3d4  yolo.pdf     9   \n",
              "96  c0fe7989-80c7-4519-99f9-7f45325d1a7f  yolo.pdf    10   \n",
              "97  51341021-c471-48ae-b2b8-29dcb72486d9  yolo.pdf    10   \n",
              "98  cd987d5a-e5c6-4708-a1b4-9b9202156266  yolo.pdf    10   \n",
              "\n",
              "                                              content  \n",
              "0   You Only Look Once:\\nUniﬁed, Real-Time Object ...  \n",
              "1   associated class probabilities. A single neura...  \n",
              "2   still achieving double the mAP of other real-t...  \n",
              "3   jects are in the image, where they are, and ho...  \n",
              "4   for general purpose, responsive robotic system...  \n",
              "..                                                ...  \n",
              "94  5\\n[28] S. Ren, K. He, R. Girshick, and J. Sun...  \n",
              "95  Recognition Challenge. International Journal o...  \n",
              "96  [33] Z. Shen and X. Xue. Do more dropouts in p...  \n",
              "97  4\\n[36] P. Viola and M. Jones. Robust real-tim...  \n",
              "98  2497–2504. IEEE, 2014. 5, 6\\n[39] C. L. Zitnic...  \n",
              "\n",
              "[99 rows x 4 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convert_vectordb_to_df(loaded_vector_db_from_local)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dCN53VfVqws"
      },
      "source": [
        "### 7. Adding a new document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu9twEE3Vqws"
      },
      "outputs": [],
      "source": [
        "# Loading new file - selenium_documentation_0.pdf\n",
        "loader = PyPDFLoader(\"selenium_documentation_0.pdf\")\n",
        "documents_new_doc = loader.load_and_split()\n",
        "\n",
        "# Splitting the documents\n",
        "split_text = RecursiveCharacterTextSplitter(chunk_size =512, chunk_overlap = 50)\n",
        "texts_new_docs = split_text.split_documents(documents=documents_new_doc)\n",
        "\n",
        "# Embedding the documents : using the same embedding used above\n",
        "\n",
        "# embeddings = HuggingFaceEmbeddings(\n",
        "#     model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "#     model_kwargs={'device':'cpu'}\n",
        "#     )\n",
        "\n",
        "faiss_index_new = FAISS.from_documents(texts_new_docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi2di_Z2Vqws"
      },
      "outputs": [],
      "source": [
        "# Loading the original vector db stored in local\n",
        "loaded_vector_db_from_local = FAISS.load_local('./faiss_index_updated', embeddings)\n",
        "\n",
        "# Merging the new vector db with the old one\n",
        "loaded_vector_db_from_local.merge_from(faiss_index_new)\n",
        "\n",
        "# Saving the merged vector db\n",
        "loaded_vector_db_from_local.save_local('./faiss_index_updated')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0r3NfWkaVqws",
        "outputId": "1f16f839-c2ce-4b3a-84dd-8f2a7570642b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                 chunk_id                      document  page  \\\n",
            "0    75337173-28cd-44ca-b57f-6b38c307a6e1                      yolo.pdf     1   \n",
            "1    ec546c79-8340-445f-ad01-b42d1db9e5b3                      yolo.pdf     1   \n",
            "2    2ce49afc-d253-43fc-95e0-f80e6ba3d2a1                      yolo.pdf     1   \n",
            "3    71bccb92-f842-44f7-b49c-efa8dc0ace50                      yolo.pdf     1   \n",
            "4    d0d2901e-8048-46c3-be4a-84ffe589717b                      yolo.pdf     1   \n",
            "..                                    ...                           ...   ...   \n",
            "844  b3cf81c2-4d76-4ea0-ae05-9efb6b0d7665  selenium_documentation_0.pdf   200   \n",
            "845  ace550c0-1f11-443c-be9c-f802e7317829  selenium_documentation_0.pdf   200   \n",
            "846  f6045114-dbad-404b-8b55-8cebcd8bbab2  selenium_documentation_0.pdf   200   \n",
            "847  df3a6312-dc71-46ba-b728-c5e2c1979b56  selenium_documentation_0.pdf   200   \n",
            "848  f03ed8b3-7dc5-44ab-8b65-22c6f2a8ee79  selenium_documentation_0.pdf   201   \n",
            "\n",
            "                                               content  \n",
            "0    You Only Look Once:\\nUniﬁed, Real-Time Object ...  \n",
            "1    associated class probabilities. A single neura...  \n",
            "2    still achieving double the mAP of other real-t...  \n",
            "3    jects are in the image, where they are, and ho...  \n",
            "4    for general purpose, responsive robotic system...  \n",
            "..                                                 ...  \n",
            "844  WebDriverBackedSelenium and use a Sizzle locat...  \n",
            "845  is no longer possible. How can you tell if you...  \n",
            "846  or “document” directly.\\nAlternatively, you mi...  \n",
            "847  \"return arguments[0].tagName\" , element);\\nNot...  \n",
            "848  Selenium Documentation, Release 1.0\\nString ti...  \n",
            "\n",
            "[849 rows x 4 columns]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>chunk_id</th>\n",
              "      <th>document</th>\n",
              "      <th>page</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>75337173-28cd-44ca-b57f-6b38c307a6e1</td>\n",
              "      <td>yolo.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>You Only Look Once:\\nUniﬁed, Real-Time Object ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ec546c79-8340-445f-ad01-b42d1db9e5b3</td>\n",
              "      <td>yolo.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>associated class probabilities. A single neura...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2ce49afc-d253-43fc-95e0-f80e6ba3d2a1</td>\n",
              "      <td>yolo.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>still achieving double the mAP of other real-t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>71bccb92-f842-44f7-b49c-efa8dc0ace50</td>\n",
              "      <td>yolo.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>jects are in the image, where they are, and ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>d0d2901e-8048-46c3-be4a-84ffe589717b</td>\n",
              "      <td>yolo.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>for general purpose, responsive robotic system...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>844</th>\n",
              "      <td>b3cf81c2-4d76-4ea0-ae05-9efb6b0d7665</td>\n",
              "      <td>selenium_documentation_0.pdf</td>\n",
              "      <td>200</td>\n",
              "      <td>WebDriverBackedSelenium and use a Sizzle locat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>845</th>\n",
              "      <td>ace550c0-1f11-443c-be9c-f802e7317829</td>\n",
              "      <td>selenium_documentation_0.pdf</td>\n",
              "      <td>200</td>\n",
              "      <td>is no longer possible. How can you tell if you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>846</th>\n",
              "      <td>f6045114-dbad-404b-8b55-8cebcd8bbab2</td>\n",
              "      <td>selenium_documentation_0.pdf</td>\n",
              "      <td>200</td>\n",
              "      <td>or “document” directly.\\nAlternatively, you mi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>847</th>\n",
              "      <td>df3a6312-dc71-46ba-b728-c5e2c1979b56</td>\n",
              "      <td>selenium_documentation_0.pdf</td>\n",
              "      <td>200</td>\n",
              "      <td>\"return arguments[0].tagName\" , element);\\nNot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>848</th>\n",
              "      <td>f03ed8b3-7dc5-44ab-8b65-22c6f2a8ee79</td>\n",
              "      <td>selenium_documentation_0.pdf</td>\n",
              "      <td>201</td>\n",
              "      <td>Selenium Documentation, Release 1.0\\nString ti...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>849 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 chunk_id                      document  page  \\\n",
              "0    75337173-28cd-44ca-b57f-6b38c307a6e1                      yolo.pdf     1   \n",
              "1    ec546c79-8340-445f-ad01-b42d1db9e5b3                      yolo.pdf     1   \n",
              "2    2ce49afc-d253-43fc-95e0-f80e6ba3d2a1                      yolo.pdf     1   \n",
              "3    71bccb92-f842-44f7-b49c-efa8dc0ace50                      yolo.pdf     1   \n",
              "4    d0d2901e-8048-46c3-be4a-84ffe589717b                      yolo.pdf     1   \n",
              "..                                    ...                           ...   ...   \n",
              "844  b3cf81c2-4d76-4ea0-ae05-9efb6b0d7665  selenium_documentation_0.pdf   200   \n",
              "845  ace550c0-1f11-443c-be9c-f802e7317829  selenium_documentation_0.pdf   200   \n",
              "846  f6045114-dbad-404b-8b55-8cebcd8bbab2  selenium_documentation_0.pdf   200   \n",
              "847  df3a6312-dc71-46ba-b728-c5e2c1979b56  selenium_documentation_0.pdf   200   \n",
              "848  f03ed8b3-7dc5-44ab-8b65-22c6f2a8ee79  selenium_documentation_0.pdf   201   \n",
              "\n",
              "                                               content  \n",
              "0    You Only Look Once:\\nUniﬁed, Real-Time Object ...  \n",
              "1    associated class probabilities. A single neura...  \n",
              "2    still achieving double the mAP of other real-t...  \n",
              "3    jects are in the image, where they are, and ho...  \n",
              "4    for general purpose, responsive robotic system...  \n",
              "..                                                 ...  \n",
              "844  WebDriverBackedSelenium and use a Sizzle locat...  \n",
              "845  is no longer possible. How can you tell if you...  \n",
              "846  or “document” directly.\\nAlternatively, you mi...  \n",
              "847  \"return arguments[0].tagName\" , element);\\nNot...  \n",
              "848  Selenium Documentation, Release 1.0\\nString ti...  \n",
              "\n",
              "[849 rows x 4 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convert_vectordb_to_df(loaded_vector_db_from_local) ## both documents are present in this Vector DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wnufgzWVqws"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cInSybiVqws"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyD0eS_YVqws"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_FE5P2LVqws"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM5z3lyUVqwt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOycOyvKVqwt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_1KZCIUVqwt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lc6pxtN7Vqwt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km7xAQ5cVqwt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tevfVPheVqwt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30i9LPuxVqwt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf6A3prsVqwt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJUTul2eVqwt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}